{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":150545,"sourceType":"datasetVersion","datasetId":70909,"isSourceIdPinned":false},{"sourceId":12595651,"sourceType":"datasetVersion","datasetId":7955505}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üåø CropSenseAI - Clean AI Plant Health Advisor \nAn optimized system designed to leverages both image and voice inputs to deliver accurate plant health predictions.\nCombining advanced AI models with a rule-based knowledge base, it analyzes visual features and audio descriptions to \ndiagnose conditions like diseases, nutrient deficiencies, and pests. The platform ensures easy deployment without \nrelying on ONNX mobile conversion, focusing on streamlined, flexible health assessment workflows. \nCropSenseAI empowers users to monitor and maintain plant vitality with reliable, multimodal insights.","metadata":{}},{"cell_type":"markdown","source":"# üåø 1. Start with Essential Imports\n   setup and readiness","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nimport os\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"    # Hide most TF/XLA logs\nos.environ['XLA_FLAGS'] = '--xla_cpu_multi_thread_eigen=false intra_op_parallelism_threads=1'\nos.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"  # Suppress warning\nos.environ[\"LOAD_IN_4BIT_FORCE\"] = \"1\"      # Force Unsloth to try loading in 4-bit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:33:23.625723Z","iopub.execute_input":"2025-07-30T10:33:23.626406Z","iopub.status.idle":"2025-07-30T10:33:23.633587Z","shell.execute_reply.started":"2025-07-30T10:33:23.626376Z","shell.execute_reply":"2025-07-30T10:33:23.633027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìã 2. Declare Required Libraries\n Install only required modules","metadata":{}},{"cell_type":"code","source":"def install_dependencies():\n    \"\"\"Install required packages for Kaggle\"\"\"\n    packages = [\n        'torch>=2.0.0',\n        'opencv-python',\n        'pillow',\n        'librosa',\n        'matplotlib',\n        'numpy',\n        'pandas',\n        'onnx',\n        'onnxruntime',\n        'bitsandbytes',\n        'transformers>=4.36.0',\n        'accelerate',\n        'sentencepiece',\n        'protobuf==3.20.*',\n        \"trl\"\n    ]\n    \n    print(\"üîÑ Installing dependencies...\")\n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\"--no-deps\", package])\n            print(f\"‚úÖ {package} installed\")\n        except subprocess.CalledProcessError:\n            print(f\"‚ö†Ô∏è Failed to install {package}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:34:23.038009Z","iopub.execute_input":"2025-07-30T10:34:23.038291Z","iopub.status.idle":"2025-07-30T10:34:23.043625Z","shell.execute_reply.started":"2025-07-30T10:34:23.038271Z","shell.execute_reply":"2025-07-30T10:34:23.042837Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üîç 3. Set Up Unsloth Framework\n   Add unsloth setups","metadata":{}},{"cell_type":"code","source":"# Try to install Unsloth for Kaggle\nimport subprocess, sys, os\n\ntry:\n     if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n        print(\"üîÑ Installing Unsloth and Unsloth Zoo (no deps)...\")\n\n        # Install unsloth\n        subprocess.check_call([\n            sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\",\n            \"git+https://github.com/unslothai/unsloth.git\"\n        ])\n\n        # Install unsloth_zoo\n        subprocess.check_call([\n            sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\",\n            \"git+https://github.com/unslothai/unsloth_zoo.git\"\n        ])\n     else:\n        subprocess.check_call([\n            sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"unsloth\"\n        ])\n        subprocess.check_call([\n            sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"unsloth_zoo\"\n        ])\n    \n     print(\"‚úÖ Unsloth and Zoo installed successfully\")\nexcept Exception as e:\n     print(\"‚ö†Ô∏è Installation failed - using fallback\")\n     print(e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:34:27.976904Z","iopub.execute_input":"2025-07-30T10:34:27.977158Z","iopub.status.idle":"2025-07-30T10:34:43.749086Z","shell.execute_reply.started":"2025-07-30T10:34:27.977140Z","shell.execute_reply":"2025-07-30T10:34:43.748433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üì• 4. Install Dependencies\n  Commonly used ","metadata":{}},{"cell_type":"code","source":"# Install dependencies\ninstall_dependencies()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:34:52.393987Z","iopub.execute_input":"2025-07-30T10:34:52.394237Z","iopub.status.idle":"2025-07-30T10:35:12.168605Z","shell.execute_reply.started":"2025-07-30T10:34:52.394218Z","shell.execute_reply":"2025-07-30T10:35:12.167937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß† 5. Load Core Modules\n   Loading key modules","metadata":{}},{"cell_type":"code","source":"# Core imports\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport librosa\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nprint(\"‚úÖ Imported all core modules successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:36:32.246177Z","iopub.execute_input":"2025-07-30T10:36:32.247142Z","iopub.status.idle":"2025-07-30T10:36:32.252325Z","shell.execute_reply.started":"2025-07-30T10:36:32.247111Z","shell.execute_reply":"2025-07-30T10:36:32.251551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. ‚öôÔ∏è Check Unsloth & ONNX Availability\n  General system check","metadata":{}},{"cell_type":"code","source":"# Check for Unsloth\ntry:\n    from unsloth import FastLanguageModel\n    UNSLOTH_AVAILABLE = True\n    print(\"‚úÖ Unsloth available\")\nexcept ImportError:\n    UNSLOTH_AVAILABLE = False\n    print(\"‚ö†Ô∏è Using standard transformers\")\n\n# Check for ONNX\ntry:\n    import onnx\n    import onnxruntime as ort\n    ONNX_AVAILABLE = True\n    print(\"‚úÖ ONNX available\")\nexcept ImportError:\n    ONNX_AVAILABLE = False\n    print(\"‚ö†Ô∏è ONNX not available\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:36:37.122825Z","iopub.execute_input":"2025-07-30T10:36:37.123441Z","iopub.status.idle":"2025-07-30T10:36:44.409627Z","shell.execute_reply.started":"2025-07-30T10:36:37.123418Z","shell.execute_reply":"2025-07-30T10:36:44.408953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. üîß Create: PlantHealthAdvisor Logic Block\n### üåø PlantHealthAdvisor ‚Äì Class Overview \n\n - Initializes a plant health advisory system that supports image, audio, and text input for diagnosing plant conditions.\n\n - Loads AI language models using either Unsloth (4-bit optimized) or standard Hugging Face transformers for generating health assessments and advice.\n\n - Processes plant images to extract visual features such as brightness, contrast, green dominance, and color variance to assess plant health.\n\n - Transcribes audio descriptions using OpenAI Whisper, allowing voice-based plant condition reporting.\n\n - Analyzes health conditions by combining AI-generated insight with rule-based matching against known symptom-treatments for issues like fungal disease, nutrient deficiency, pests, or water stress.\n\n - Generates tailored treatment recommendations based on analysis, with confidence scores derived from image quality and condition matches.\n\n - Saves analysis results and visualizations (JSON + PNG) to a timestamped output directory for record-keeping and user-friendly summaries.\n\n - Supports ONNX model conversion for lightweight, mobile-friendly deployment of the advisory system.\n\n - Provides a demo image generator to simulate plant health conditions (e.g., healthy, sick, pest damage) for testing and training.\n\n - Designed for flexible deployment across devices, with GPU support, rule-based fallbacks, and dynamic multimodal inputs (image, audio, and text).","metadata":{}},{"cell_type":"code","source":"class PlantHealthAdvisor:\n    \"\"\"Clean, focused plant health analysis system\"\"\"\n    \n    def __init__(self):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = None\n        self.tokenizer = None\n        self.audio_processor = None\n        \n        # Create output directory\n        self.output_dir = Path(\"./plant_analysis_output\")\n        self.output_dir.mkdir(exist_ok=True)\n        \n        # Plant health knowledge base\n        self.health_conditions = {\n            'healthy': {\n                'symptoms': ['green leaves', 'upright growth', 'no discoloration'],\n                'treatment': 'Continue current care. Monitor regularly for changes.'\n            },\n            'fungal_disease': {\n                'symptoms': ['white spots', 'black spots', 'yellowing', 'powdery coating'],\n                'treatment': 'Apply fungicide spray. Improve air circulation. Reduce watering frequency.'\n            },\n            'nutrient_deficiency': {\n                'symptoms': ['yellow leaves', 'stunted growth', 'pale color', 'brown edges'],\n                'treatment': 'Apply balanced fertilizer. Check soil pH. Ensure proper nutrients.'\n            },\n            'pest_damage': {\n                'symptoms': ['holes in leaves', 'chewed edges', 'visible insects'],\n                'treatment': 'Use organic pesticide. Remove affected parts. Check for pest eggs.'\n            },\n            'water_stress': {\n                'symptoms': ['wilting', 'drooping', 'dry soil', 'brown tips'],\n                'treatment': 'Adjust watering schedule. Check soil moisture. Improve drainage.'\n            }\n        }\n        \n        print(\"üåø PlantHealthAdvisor initialized\")\n    \n    def load_model(self):\n        \"\"\"Load the best available model\"\"\"\n        try:\n            if UNSLOTH_AVAILABLE:\n                return self._load_unsloth_model()\n            else:\n                return self._load_standard_model()\n        except Exception as e:\n            logger.error(f\"Model loading failed: {e}\")\n            return False\n    \n    def _load_unsloth_model(self):\n        \"\"\"Load Unsloth model for better performance\"\"\"\n        try:\n            model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\"  # Smaller model for Kaggle\n            print(f\"üîÑ Loading Unsloth model: {model_name}\")\n            \n            self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n                model_name=model_name,\n                max_seq_length=1024,\n                load_in_4bit=True,\n            )\n            \n            FastLanguageModel.for_inference(self.model)\n            print(\"‚úÖ Unsloth model loaded successfully\")\n            return True\n            \n        except Exception as e:\n            print(f\"‚ùå Unsloth model failed: {e}\")\n            return self._load_standard_model()\n        \n    def _load_standard_model(self):\n        \"\"\"Load standard transformers model\"\"\"\n        try:\n            model_name = \"microsoft/DialoGPT-medium\"\n            print(f\"üîÑ Loading standard model: {model_name}\")\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None\n            )\n            \n            print(\"‚úÖ Standard model loaded successfully\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Standard model loading failed: {e}\")\n            return False\n    \n    def load_audio_processor(self):\n        \"\"\"Load audio processing pipeline\"\"\"\n        try:\n            self.audio_processor = pipeline(\n                \"automatic-speech-recognition\",\n                model=\"openai/whisper-tiny.en\",\n                device=0 if torch.cuda.is_available() else -1\n            )\n            print(\"‚úÖ Audio processor loaded\")\n            return True\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Audio processor failed: {e}\")\n            return False\n    \n    def process_image(self, image_path):\n        \"\"\"Extract features from plant image\"\"\"\n        try:\n            # Load and resize image\n            image = Image.open(image_path).convert('RGB')\n            image_resized = image.resize((224, 224))\n            image_array = np.array(image_resized)\n            \n            # Extract basic visual features\n            features = {\n                'brightness': float(np.mean(image_array)),\n                'contrast': float(np.std(image_array)),\n                'green_dominance': float(np.mean(image_array[:,:,1]) / np.mean(image_array)),\n                'color_variance': float(np.var(image_array.reshape(-1, 3), axis=0).mean()),\n                'image_shape': image.size\n            }\n            \n            return features, image\n            \n        except Exception as e:\n            logger.error(f\"Image processing failed: {e}\")\n            return None, None\n    \n    def process_audio(self, audio_path):\n        \"\"\"Convert audio to text description\"\"\"\n        try:\n            if self.audio_processor is None:\n                if not self.load_audio_processor():\n                    return \"Audio processing not available\"\n            \n            # Process audio file\n            result = self.audio_processor(audio_path)\n            text = result.get('text', '')\n            \n            print(f\"üé§ Audio transcribed: {text}\")\n            return text\n            \n        except Exception as e:\n            logger.error(f\"Audio processing failed: {e}\")\n            return f\"Audio processing error: {str(e)}\"\n    \n    def analyze_plant_health(self, image_path, audio_path=None, text_description=\"\"):\n        \"\"\"Main analysis function\"\"\"\n        try:\n            print(\"üåø Starting plant health analysis...\")\n            \n            # Process image\n            image_features, image = self.process_image(image_path)\n            if image_features is None:\n                return {\"error\": \"Failed to process image\"}\n            \n            # Process audio if provided\n            audio_text = \"\"\n            if audio_path and os.path.exists(audio_path):\n                audio_text = self.process_audio(audio_path)\n            \n            # Combine descriptions\n            full_description = f\"{text_description} {audio_text}\".strip()\n            \n            # Generate AI analysis\n            analysis = self._generate_analysis(image_features, full_description)\n            \n            # Get recommendations\n            recommendations = self._get_recommendations(analysis, image_features)\n            \n            # Create result\n            result = {\n                'timestamp': datetime.now().isoformat(),\n                'image_path': str(image_path),\n                'audio_path': str(audio_path) if audio_path else None,\n                'image_features': image_features,\n                'description': full_description,\n                'analysis': analysis,\n                'recommendations': recommendations,\n                'confidence': self._calculate_confidence(analysis, image_features)\n            }\n            \n            # Save results\n            self._save_results(result)\n            \n            # Create visualization\n            self._create_visualization(image, result)\n            \n            print(\"‚úÖ Analysis complete!\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"Analysis failed: {e}\")\n            return {\"error\": str(e)}\n    \n    def _generate_analysis(self, image_features, description):\n        \"\"\"Generate AI analysis of plant health\"\"\"\n        try:\n            if self.model is None:\n                return \"Model not loaded - using rule-based analysis\"\n            \n            # Create analysis prompt\n            prompt = f\"\"\"\n            Plant Health Analysis:\n            \n            Image Analysis:\n            - Brightness: {image_features['brightness']:.1f}\n            - Contrast: {image_features['contrast']:.1f}\n            - Green dominance: {image_features['green_dominance']:.2f}\n            - Color variance: {image_features['color_variance']:.1f}\n            \n            Description: {description}\n            \n            Based on this information, analyze the plant's health status, identify any issues, and suggest the most likely condition. Be concise and specific.\n            \n            Analysis:\"\"\"\n            \n            # Generate response\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n            \n            if torch.cuda.is_available():\n                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=200,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode response\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            analysis = response.replace(prompt, \"\").strip()\n            \n            return analysis if analysis else \"Unable to generate detailed analysis\"\n            \n        except Exception as e:\n            logger.error(f\"AI analysis failed: {e}\")\n            return self._rule_based_analysis(image_features, description)\n    \n    def _rule_based_analysis(self, image_features, description):\n        \"\"\"Fallback rule-based analysis\"\"\"\n        analysis = []\n        \n        # Analyze image features\n        if image_features['green_dominance'] < 0.9:\n            analysis.append(\"Low green coloration detected - possible health issues\")\n        \n        if image_features['brightness'] < 100:\n            analysis.append(\"Image appears dark - may indicate poor lighting conditions\")\n        \n        if image_features['contrast'] < 30:\n            analysis.append(\"Low contrast - image quality may affect analysis\")\n        \n        # Analyze description\n        description_lower = description.lower()\n        for condition, info in self.health_conditions.items():\n            for symptom in info['symptoms']:\n                if symptom in description_lower:\n                    analysis.append(f\"Symptoms suggest possible {condition.replace('_', ' ')}\")\n                    break\n        \n        return \". \".join(analysis) if analysis else \"Plant appears to be in normal condition based on available data\"\n    \n    def _get_recommendations(self, analysis, image_features):\n        \"\"\"Generate specific recommendations\"\"\"\n        recommendations = []\n        analysis_lower = analysis.lower()\n        \n        # Match analysis with known conditions\n        for condition, info in self.health_conditions.items():\n            if condition.replace('_', ' ') in analysis_lower:\n                recommendations.append({\n                    'condition': condition,\n                    'treatment': info['treatment'],\n                    'confidence': 0.8\n                })\n        \n        # Add general recommendations based on image features\n        if image_features['brightness'] < 80:\n            recommendations.append({\n                'condition': 'lighting',\n                'treatment': 'Consider providing more light or moving to brighter location',\n                'confidence': 0.6\n            })\n        \n        if image_features['green_dominance'] < 0.85:\n            recommendations.append({\n                'condition': 'general_health',\n                'treatment': 'Monitor plant closely and check for signs of stress or disease',\n                'confidence': 0.7\n            })\n        \n        # Default recommendation if none found\n        if not recommendations:\n            recommendations.append({\n                'condition': 'maintenance',\n                'treatment': 'Continue regular care routine. Monitor for any changes in plant health.',\n                'confidence': 0.5\n            })\n        \n        return recommendations\n    \n    def _calculate_confidence(self, analysis, image_features):\n        \"\"\"Calculate confidence score for the analysis\"\"\"\n        confidence = 0.5  # Base confidence\n        \n        # Increase confidence based on image quality\n        if image_features['brightness'] > 80:\n            confidence += 0.1\n        if image_features['contrast'] > 40:\n            confidence += 0.1\n        \n        # Increase confidence if specific conditions are identified\n        if any(condition in analysis.lower() for condition in self.health_conditions.keys()):\n            confidence += 0.2\n        \n        return min(confidence, 0.95)  # Cap at 95%\n    \n    def _save_results(self, result):\n        \"\"\"Save analysis results to JSON\"\"\"\n        try:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            results_file = self.output_dir / f\"analysis_{timestamp}.json\"\n            \n            with open(results_file, 'w') as f:\n                json.dump(result, f, indent=2, default=str)\n            \n            print(f\"üíæ Results saved: {results_file}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to save results: {e}\")\n    \n    def _create_visualization(self, image, result):\n        \"\"\"Create analysis visualization\"\"\"\n        try:\n            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n            \n            # Display image\n            axes[0].imshow(image)\n            axes[0].set_title('Plant Image')\n            axes[0].axis('off')\n            \n            # Display analysis results\n            analysis_text = f\"\"\"\n            PLANT HEALTH ANALYSIS\n            \n            Confidence: {result['confidence']:.1%}\n            \n            Analysis:\n            {result['analysis'][:200]}...\n            \n            Key Recommendations:\n            \"\"\"\n            \n            for i, rec in enumerate(result['recommendations'][:3], 1):\n                analysis_text += f\"\\n{i}. {rec['treatment'][:50]}...\"\n            \n            axes[1].text(0.05, 0.95, analysis_text, transform=axes[1].transAxes,\n                        fontsize=9, verticalalignment='top', fontfamily='monospace',\n                        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n            axes[1].set_title('Analysis Results')\n            axes[1].axis('off')\n            \n            plt.tight_layout()\n            \n            # Save visualization\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            viz_file = self.output_dir / f\"visualization_{timestamp}.png\"\n            plt.savefig(viz_file, dpi=150, bbox_inches='tight')\n            plt.show()\n            \n            print(f\"üìä Visualization saved: {viz_file}\")\n            \n        except Exception as e:\n            logger.error(f\"Visualization failed: {e}\")\n    \n    def convert_to_onnx(self):\n        \"\"\"Convert model to ONNX for mobile deployment\"\"\"\n        try:\n            if not ONNX_AVAILABLE:\n                print(\"‚ùå ONNX not available\")\n                return False\n            \n            if self.model is None:\n                print(\"‚ùå No model loaded\")\n                return False\n            \n            print(\"üîÑ Converting model to ONNX...\")\n            \n            # Prepare sample input\n            sample_text = \"Analyze this plant for health issues and diseases.\"\n            inputs = self.tokenizer(sample_text, return_tensors=\"pt\", max_length=512, truncation=True)\n            \n            # Set model to evaluation mode\n            self.model.eval()\n            \n            # Export to ONNX\n            onnx_path = self.output_dir / \"plant_health_model.onnx\"\n            \n            torch.onnx.export(\n                self.model,\n                inputs['input_ids'],\n                onnx_path,\n                export_params=True,\n                opset_version=11,\n                do_constant_folding=True,\n                input_names=['input_ids'],\n                output_names=['logits'],\n                dynamic_axes={\n                    'input_ids': {0: 'batch_size', 1: 'sequence'},\n                    'logits': {0: 'batch_size', 1: 'sequence'}\n                }\n            )\n            \n            # Verify ONNX model\n            onnx_model = onnx.load(onnx_path)\n            onnx.checker.check_model(onnx_model)\n            \n            print(f\"‚úÖ ONNX model saved: {onnx_path}\")\n            print(f\"üì± Model size: {os.path.getsize(onnx_path) / (1024*1024):.1f} MB\")\n            \n            return str(onnx_path)\n            \n        except Exception as e:\n            logger.error(f\"ONNX conversion failed: {e}\")\n            return False\n    \n    def create_demo_image(self, condition=\"healthy\"):\n        \"\"\"Create a demo plant image for testing\"\"\"\n        from PIL import ImageDraw\n        import random\n        \n        # Create image\n        img = Image.new('RGB', (300, 400), color='lightblue')\n        draw = ImageDraw.Draw(img)\n        \n        # Draw pot\n        draw.ellipse([50, 320, 250, 380], fill='brown')\n        \n        # Draw stem\n        draw.rectangle([145, 200, 155, 320], fill='darkgreen')\n        \n        # Draw leaves based on condition\n        leaf_color = 'green'\n        if condition == 'sick':\n            leaf_color = 'yellow'\n        elif condition == 'pest_damage':\n            leaf_color = 'darkgreen'\n        \n        # Draw multiple leaves\n        for i in range(6):\n            x = 150 + random.randint(-60, 60)\n            y = 120 + i * 25\n            draw.ellipse([x-25, y-10, x+25, y+10], fill=leaf_color)\n            \n            # Add damage for sick plants\n            if condition == 'sick':\n                draw.ellipse([x-5, y-3, x+5, y+3], fill='brown')\n            elif condition == 'pest_damage':\n                draw.ellipse([x-3, y-2, x+3, y+2], fill='black')\n        \n        return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:36:51.488304Z","iopub.execute_input":"2025-07-30T10:36:51.488983Z","iopub.status.idle":"2025-07-30T10:36:51.524122Z","shell.execute_reply.started":"2025-07-30T10:36:51.488954Z","shell.execute_reply":"2025-07-30T10:36:51.523435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. üö¶ Run Analysis Example\n### Main Execution Functions Overview\n- run_analysis initializes the PlantHealthAdvisor, loads the AI model, and runs a comprehensive plant health analysis combining image, audio, and text inputs.\n\n- It handles errors gracefully, printing detailed analysis results including confidence scores and treatment recommendations.\n\n- demo_with_sample_image generates demo plant images representing healthy and sick conditions, saves them locally, and runs analyses on both samples.\n\n- The demo function showcases end-to-end functionality from image creation to AI-powered health assessment with descriptive outputs.\n\n- Both functions optionally support converting the loaded AI model to ONNX format for mobile deployment and lightweight use.","metadata":{}},{"cell_type":"code","source":"# Main execution functions\ndef run_analysis(image_path, audio_path=None, description=\"\"):\n    \"\"\"Run complete plant health analysis\"\"\"\n    \n    # Initialize advisor\n    advisor = PlantHealthAdvisor()\n    \n    # Load model\n    print(\"üìö Loading AI model...\")\n    if not advisor.load_model():\n        print(\"‚ö†Ô∏è Model loading failed - using fallback analysis\")\n    \n    # Run analysis\n    result = advisor.analyze_plant_health(image_path, audio_path, description)\n    \n    if 'error' in result:\n        print(f\"‚ùå Analysis failed: {result['error']}\")\n        return None\n    \n    # Display results\n    print(\"\\n\" + \"=\"*60)\n    print(\"üåø PLANT HEALTH ANALYSIS RESULTS\")\n    print(\"=\"*60)\n    print(f\"üì∏ Image: {result['image_path']}\")\n    print(f\"üé§ Audio: {result['audio_path'] or 'None'}\")\n    print(f\"üí¨ Description: {result['description']}\")\n    print(f\"üéØ Confidence: {result['confidence']:.1%}\")\n    \n    print(f\"\\nüìã Analysis:\")\n    print(result['analysis'])\n    \n    print(f\"\\nüí° Recommendations:\")\n    for i, rec in enumerate(result['recommendations'], 1):\n        print(f\"{i}. {rec['treatment']} (Confidence: {rec['confidence']:.1%})\")\n    \n    # # Convert model to ONNX\n    # print(\"\\nüì± Converting model for mobile deployment...\")\n    # onnx_path = advisor.convert_to_onnx()\n    # if onnx_path:\n    #     print(f\"‚úÖ Mobile model ready: {onnx_path}\")\n        \n    return result\n\ndef demo_with_sample_image():\n    \"\"\"Run demo with a generated sample image\"\"\"\n    print(\"üé¨ Running VerdantVision Demo\")\n    print(\"=\"*50)\n    \n    # Create sample images\n    advisor = PlantHealthAdvisor()\n    \n    print(\"üì∑ Creating demo images...\")\n    healthy_img = advisor.create_demo_image(\"healthy\")\n    sick_img = advisor.create_demo_image(\"sick\")\n    \n    # Save demo images\n    demo_dir = Path(\"./demo_images\")\n    demo_dir.mkdir(exist_ok=True)\n    \n    healthy_path = demo_dir / \"healthy_plant.jpg\"\n    sick_path = demo_dir / \"sick_plant.jpg\"\n    \n    healthy_img.save(healthy_path)\n    sick_img.save(sick_path)\n    \n    print(f\"‚úÖ Demo images created: {healthy_path}, {sick_path}\")\n    \n    # Analyze healthy plant\n    print(\"\\nüå± Analyzing healthy plant...\")\n    result1 = run_analysis(str(healthy_path), description=\"This plant looks green and vibrant\")\n    \n    # Analyze sick plant\n    print(\"\\nü§í Analyzing sick plant...\")\n    result2 = run_analysis(str(sick_path), description=\"This plant has yellow spots and looks unhealthy\")\n    \n    # Convert model to ONNX\n    print(\"\\nüì± Converting model for mobile deployment...\")\n    onnx_path = advisor.convert_to_onnx()\n    if onnx_path:\n        print(f\"‚úÖ Mobile model ready: {onnx_path}\")\n    \n    return result1, result2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T11:39:33.827594Z","iopub.execute_input":"2025-07-30T11:39:33.827935Z","iopub.status.idle":"2025-07-30T11:39:33.837061Z","shell.execute_reply.started":"2025-07-30T11:39:33.827911Z","shell.execute_reply":"2025-07-30T11:39:33.836202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. üîÅ Test Run Example\n### System Initialization and Usage Overview\n- Displays key system info including Python version, PyTorch, CUDA availability, Unsloth and ONNX support to confirm environment readiness.\n\n- Prints clear usage examples demonstrating how to run plant health analysis with image only, image plus description, image plus audio, or a full demo.\n\n- Provides users a quick start guide to interact with the PlantHealthAdvisor system confidently and efficiently.","metadata":{}},{"cell_type":"code","source":"\n# Initialize and display system info\nprint(\"üåø VerdantVision: Clean Plant Health Advisor\")\nprint(\"=\"*60)\nprint(f\"üêç Python: {sys.version_info.major}.{sys.version_info.minor}\")\nprint(f\"üî• PyTorch: {torch.__version__}\")\nprint(f\"üíæ CUDA: {torch.cuda.is_available()}\")\nprint(f\"üöÄ Unsloth: {UNSLOTH_AVAILABLE}\")\nprint(f\"üì± ONNX: {ONNX_AVAILABLE}\")\nprint(\"\\n‚úÖ Ready for plant health analysis!\")\n\n# Usage examples:\nprint(\"\\nüìñ Usage Examples:\")\nprint(\"1. Analyze with image only:\")\nprint(\"   result = run_analysis('path/to/plant.jpg')\")\nprint(\"\\n2. Analyze with image and description:\")\nprint(\"   result = run_analysis('path/to/plant.jpg', description='Plant has yellow leaves')\")\nprint(\"\\n3. Analyze with image and audio:\")\nprint(\"   result = run_analysis('path/to/plant.jpg', 'path/to/audio.wav')\")\nprint(\"\\n4. Run demo:\")\nprint(\"   demo_with_sample_image()\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T11:39:42.790623Z","iopub.execute_input":"2025-07-30T11:39:42.790938Z","iopub.status.idle":"2025-07-30T11:39:42.797502Z","shell.execute_reply.started":"2025-07-30T11:39:42.790914Z","shell.execute_reply":"2025-07-30T11:39:42.796662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. üåø Example: Full Plant Health Analysis with Image and Audio\n- Runs a full plant health analysis on the specified image and accompanying audio description.\n\n- Returns detailed diagnostics, confidence scores, and treatment recommendations from the PlantHealthAdvisor system.","metadata":{}},{"cell_type":"code","source":"result = run_analysis('/kaggle/input/testdata/4-_philodendron_pdic.jpg', '/kaggle/input/testdata/plant_question_offline.wav')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T11:39:53.297402Z","iopub.execute_input":"2025-07-30T11:39:53.298230Z","iopub.status.idle":"2025-07-30T11:40:04.592924Z","shell.execute_reply.started":"2025-07-30T11:39:53.298196Z","shell.execute_reply":"2025-07-30T11:40:04.592195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}